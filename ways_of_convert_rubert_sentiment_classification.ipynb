{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starminalush/mlops_report/blob/main/ways_of_convert_rubert_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Введение"
      ],
      "metadata": {
        "id": "NCqsfpgGKl0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот ноутбук для бекендеров, которым дали модельку и сказали деплоить так, чтобы она быстро работала. И больше ничего не дали, кроме модельки\n",
        "\t\n",
        "  (・_・ヾ"
      ],
      "metadata": {
        "id": "swO2xuTUxKuj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIhzpUzi6O93"
      },
      "source": [
        "Устанавливаем нужные зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98Fnu0cUxT-B",
        "outputId": "5d7c799e-179e-4f38-8c82-080417e90b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 15.0 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 48.8 MB/s \n",
            "\u001b[?25hCollecting onnxruntime-gpu==1.11.0\n",
            "  Downloading onnxruntime_gpu-1.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (108.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 108.9 MB 5.1 kB/s \n",
            "\u001b[?25hCollecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting optimum[onnxruntime]\n",
            "  Downloading optimum-1.1.1.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime-gpu==1.11.0) (2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from onnxruntime-gpu==1.11.0) (1.21.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime-gpu==1.11.0) (3.17.3)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime-gpu==1.11.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 54.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.7/dist-packages (from optimum[onnxruntime]) (1.11.0+cu113)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from optimum[onnxruntime]) (1.7.1)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.11.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 36.7 MB/s \n",
            "\u001b[?25hCollecting datasets>=1.2.1\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 76.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (6.0.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 18.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 75.4 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (21.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 69.7 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (2.0.12)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2022.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->optimum[onnxruntime]) (1.2.1)\n",
            "Building wheels for collected packages: folium, optimum, sacremoses\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=9b7bc8c446f76dbc272cf2e1e85fda551bb239d878421bd5f0ecdcbece5327fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "  Building wheel for optimum (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum: filename=optimum-1.1.1-py3-none-any.whl size=83821 sha256=9a794e09e1686f1b4ccae48c1a354644356e4523607a0badb7b928daf1f40132\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/fd/0e/f2d8b5936fd9a3b231b0146ad0233d0572879e5a22378c9a16\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ac1bed7d4489ffc35b049c4fdc1b104d19be3145f8f266a28f205405371e5cff\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built folium optimum sacremoses\n",
            "Installing collected packages: urllib3, multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, sacremoses, humanfriendly, huggingface-hub, fsspec, aiohttp, xxhash, transformers, responses, coloredlogs, optimum, onnxruntime, onnx, datasets, onnxruntime-gpu, folium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 coloredlogs-15.0.1 datasets-2.1.0 folium-0.2.1 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 humanfriendly-10.0 multidict-6.0.2 onnx-1.11.0 onnxruntime-1.11.1 onnxruntime-gpu-1.11.0 optimum-1.1.1 pyyaml-6.0 responses-0.18.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx transformers onnxruntime-gpu==1.11.0 folium==0.2.1 optimum[onnxruntime]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2LrH-YLj6jK"
      },
      "source": [
        "Фиксируем версии библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RJtoXVKbj43r"
      },
      "outputs": [],
      "source": [
        "!pip freeze > req.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX3P6Agw6jsm"
      },
      "source": [
        "Импорты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UPvsKtuB6iyY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers.onnx import export\n",
        "from pathlib import Path\n",
        "from typing import Mapping, OrderedDict\n",
        "from transformers.onnx import OnnxConfig\n",
        "from transformers import AutoConfig\n",
        "import onnxruntime as nxrun\n",
        "import onnx\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "from torch.nn.utils import prune\n",
        "from optimum.onnxruntime import ORTQuantizer\n",
        "from torch.onnx import TrainingMode\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Качаем датасет, на котором будем проверять качество модели"
      ],
      "metadata": {
        "id": "PeVa2PBwZ-63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/sismetanin/rureviews/raw/master/women-clothing-accessories.3-class.balanced.csv"
      ],
      "metadata": {
        "id": "bswJJzJwZ-ZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f4d621-d1fd-4b3a-ca47-56bb00c40e12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-03 17:54:40--  https://github.com/sismetanin/rureviews/raw/master/women-clothing-accessories.3-class.balanced.csv\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sismetanin/rureviews/master/women-clothing-accessories.3-class.balanced.csv [following]\n",
            "--2022-05-03 17:54:40--  https://raw.githubusercontent.com/sismetanin/rureviews/master/women-clothing-accessories.3-class.balanced.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21781685 (21M) [text/plain]\n",
            "Saving to: ‘women-clothing-accessories.3-class.balanced.csv.1’\n",
            "\n",
            "women-clothing-acce 100%[===================>]  20.77M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-03 17:54:41 (184 MB/s) - ‘women-clothing-accessories.3-class.balanced.csv.1’ saved [21781685/21781685]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Об нейросети"
      ],
      "metadata": {
        "id": "bN3PMwCwKu8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве подопытного будем использовать [rubert-base-cased-sentiment](https://huggingface.co/blanchefort/rubert-base-cased-sentiment) для классификации русских предложений. Данная нейросеть предсказывает 3 метки класса, в зависимости от тона предложения - позитивное, негативное или нейтральное"
      ],
      "metadata": {
        "id": "OnWsmqJ8JEID"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCV1_Y3hjr4C"
      },
      "source": [
        "Запускаем нейросеть как есть"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y68sxMotjS-K"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment', return_dict=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(text, device):\n",
        "    model_local = model.to(device)\n",
        "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    outputs = model_local(**inputs)\n",
        "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    predicted = torch.argmax(predicted, dim=1).cpu().numpy()\n",
        "    return predicted[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Как задолбали эти тупые правила: не есть кота, не бить посуду, не есть кота'"
      ],
      "metadata": {
        "id": "j3WKSWz8bsoJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим время инференса модели на gpu и cpu"
      ],
      "metadata": {
        "id": "tj7kkrS1R2KL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "halOX7lokqyT",
        "outputId": "7e49d1a2-c834-4a5c-82dc-98afd6368462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 113 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "predict(text, device = torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "predict(text, device = torch.device('cuda'))"
      ],
      "metadata": {
        "id": "VbLhAo36BRtO",
        "outputId": "24f4853b-c31c-4b01-f168-98def138374d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 1010.65 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 12.7 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим качество модели. Для проверки качества будем использовать один из датасетов, на котором обучалась модель, а именно [этот](https://github.com/sismetanin/rureviews)"
      ],
      "metadata": {
        "id": "jze-C8D-R6jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/women-clothing-accessories.3-class.balanced.csv', delimiter='\\t')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Blvw3zQqVSZS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "91ec3f8b-a5de-40b3-f97b-61ac59e9794f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  качество плохое пошив ужасный (горловина напер...  negative\n",
              "1  Товар отдали другому человеку, я не получила п...  negative\n",
              "2  Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
              "3  товар не пришел, продавец продлил защиту без м...  negative\n",
              "4      Кофточка голая синтетика, носить не возможно.  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d16edad8-42b9-40b9-9d06-3edfba661d44\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>качество плохое пошив ужасный (горловина напер...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Товар отдали другому человеку, я не получила п...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ужасная синтетика! Тонкая, ничего общего с пре...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>товар не пришел, продавец продлил защиту без м...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Кофточка голая синтетика, носить не возможно.</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d16edad8-42b9-40b9-9d06-3edfba661d44')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d16edad8-42b9-40b9-9d06-3edfba661d44 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d16edad8-42b9-40b9-9d06-3edfba661d44');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для удобства немного изменим датасет - заменим метки класса на цифровые значения и выберем 1000 рандомных строк"
      ],
      "metadata": {
        "id": "ul8Y52nOVx_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df = df[:1000]\n",
        "mapping = {'negative': 2, 'positive': 1, 'neautral':0}\n",
        "df = df.replace({'sentiment': mapping})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3csvwJ45V6SO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ae728d74-1bae-4f7c-9a33-4944b7e682ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  Очень хорошая куртка, довольно таки плотная. Я...          1\n",
              "1  Заказ так и не пришёл, деньги не вернули. Спор...          2\n",
              "2                                             Пришло          0\n",
              "3  Заказывала S. Размер подошёл. Но есть одно но,...          0\n",
              "4  на спине между лямками дырка. продавец ничего ...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-422155f6-e7cd-4522-99ad-d664578768c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Очень хорошая куртка, довольно таки плотная. Я...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Заказ так и не пришёл, деньги не вернули. Спор...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Пришло</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Заказывала S. Размер подошёл. Но есть одно но,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>на спине между лямками дырка. продавец ничего ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-422155f6-e7cd-4522-99ad-d664578768c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-422155f6-e7cd-4522-99ad-d664578768c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-422155f6-e7cd-4522-99ad-d664578768c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем качество"
      ],
      "metadata": {
        "id": "QwNFigwRX451"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(df['review'])\n",
        "labels = list(df['sentiment'])"
      ],
      "metadata": {
        "id": "sziEW5X3X8qD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict(text=t, device=torch.device('cuda')) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZg4zSOAYSAk",
        "outputId": "d2babf22-fc81-42ee-b106-928a3c7dbb64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7570952595655819, recall: 0.7528932881996591, f1score: 0.7418477839235202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраним оригинальную модель и посмотрим на ее вес"
      ],
      "metadata": {
        "id": "YCTWXOk829Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output"
      ],
      "metadata": {
        "id": "uNKv-i2QcfGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24c2993-c6f2-4cb3-8fad-f86724cb8e0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘output’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'output/original.pt')"
      ],
      "metadata": {
        "id": "mtpd-cj53F_V"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -shc output/original.pt"
      ],
      "metadata": {
        "id": "Jy4Ok3jw3OUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3d5e8e-06d6-4b0a-ebf5-cbf3418de7a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679M\toutput/original.pt\n",
            "679M\ttotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX"
      ],
      "metadata": {
        "id": "ITuXOm0CZ0Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формат Open Neural Network Exchange (ONNX) обеспечит общий способ представления данных, используемых в нейронных сетях. Большинство платформ имеют сегодня собственный специфический формат моделей, которые способны работать с моделями других платформ только при использовании специальных инструментов преобразования форматов.\n",
        "\n",
        "ONNX позволит осуществлять свободный обмен информацией, которой обладают модели, без процедуры преобразования. Модель, обученную на одной платформе, можно будет использовать и на другой платформе. Также можно будет модель, обученную на одном фреймворке, перенести на другой фреймворк.\n",
        "\n",
        "Перевести модель в ONNX можно несколькими способами:"
      ],
      "metadata": {
        "id": "p0aL7IAQaHje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Есть способ конвертации модели через torch.onnx"
      ],
      "metadata": {
        "id": "goPzxMVnadSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S6uO6quNkUSn"
      },
      "outputs": [],
      "source": [
        "!mkdir -p output/onnx_transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#делаем dummy input\n",
        "dummy_input0 = torch.randint(1, 224, (1,512))\n",
        "dummy_input1 = torch.randint(0, 1, (1,512 ))\n",
        "dummy_input2 =  torch.randint(0, 1, (1,512 ))\n",
        "dummy_inputs = (dummy_input0,dummy_input1,dummy_input2)\n",
        "device = torch.device('cpu')\n",
        "with torch.no_grad():\n",
        "  symbolic_names = {0:'batch_size', 1: 'max_seq_len'} \n",
        "  torch.onnx.export(model.to(device),               # модель, которую будем экспортировать\n",
        "                    dummy_inputs,                         # input модели\n",
        "                    \"output/onnx_transforms/rubert-base-cased-sentiment_torch.onnx\",   # путь сохранения модель\n",
        "                    export_params=True,        \n",
        "                    opset_version=11,          # версия ONNX, в который будем экспортировать модель\n",
        "                    do_constant_folding=True,\n",
        "                    input_names = [\"input_ids\",\"attention_mask\",\"token_type_ids\"],\n",
        "                    output_names = ['output'],\n",
        "                    dynamic_axes={'input_ids': symbolic_names,        #если у нас динамический размер input\n",
        "                                  'attention_mask' : symbolic_names,\n",
        "                                  'token_type_ids' : symbolic_names},\n",
        "                    training=TrainingMode.EVAL\n",
        "                    )"
      ],
      "metadata": {
        "id": "AsPlz8DGauZV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMKViWTow1s_"
      },
      "source": [
        "Пробуем запустить в ONNX на gpu и cpu и посмотреть время инференса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CeVlwXC8xbpj"
      },
      "outputs": [],
      "source": [
        "sess_options = nxrun.SessionOptions()\n",
        "# запуск на gpu\n",
        "providers = [\n",
        "    'CUDAExecutionProvider'\n",
        "]\n",
        "\n",
        "model_ONNX = nxrun.InferenceSession(\"output/onnx_transforms/rubert-base-cased-sentiment_torch.onnx\", sess_options, providers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iAu2HFt6x9WM"
      },
      "outputs": [],
      "source": [
        "def predict_onnx(text):\n",
        "  inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='np')\n",
        "  outputs  = model_ONNX.run(None, dict(inputs))[0][0]\n",
        "  result = np.where(outputs == np.amax(outputs))[0][0]\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "d76ecT6N0a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2157f7d-e766-40af-b0a7-6c41e1a915a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 13.51 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "100 loops, best of 5: 3.6 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit \n",
        "predict_onnx(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#запуск на cpu\n",
        "providers = [\n",
        "    'CPUExecutionProvider'\n",
        "]\n",
        "\n",
        "model_ONNX = nxrun.InferenceSession(\"output/onnx_transforms/rubert-base-cased-sentiment_torch.onnx\", sess_options, providers)"
      ],
      "metadata": {
        "id": "ZYY5rWVhCCoV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit \n",
        "predict_onnx(text)"
      ],
      "metadata": {
        "id": "maNwRh-MCIym",
        "outputId": "315c9d46-3c35-4c64-cfa7-3af02ebdbe05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 54.7 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем качество"
      ],
      "metadata": {
        "id": "rdcziw6HeR6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_onnx(text = t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "id": "_t96PNPob8I0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eafcaf77-5dbe-4423-b747-e538d8bbec07"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.779944967747291, recall: 0.7718710037195792, f1score: 0.7701675746159881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на вес модели"
      ],
      "metadata": {
        "id": "UrZPxeh_jCk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!du -shc output/onnx_transforms/*"
      ],
      "metadata": {
        "id": "BXjx9u88jFDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249b72ee-a411-44b2-8d4a-2e4168fc5a51"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679M\toutput/onnx_transforms/rubert-base-cased-sentiment_torch.onnx\n",
            "679M\ttotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:  по сравнению с оригинальной моделью скорость инференса модели стала на порядок выше, метрики качества не изменились"
      ],
      "metadata": {
        "id": "ncawUB5neW9b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhA84H-tlJCl"
      },
      "source": [
        "2. Есть библиотека transforms для трансформеров, [где все почти из коробки](https://huggingface.co/docs/transformers/serialization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "dXO_0DXu3e3s"
      },
      "outputs": [],
      "source": [
        "class DistilBertOnnxConfig(OnnxConfig):\n",
        "    @property\n",
        "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
        "        return OrderedDict(\n",
        "            [\n",
        "                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n",
        "                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n",
        "                (\"token_type_ids\", {0: \"batch\", 1: \"sequence\"}),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kFThnLqO3O6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75bc9955-9f20-438f-d128-f69e9e514863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('logits', {0: 'batch'})])\n"
          ]
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\")\n",
        "onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task=\"sequence-classification\")\n",
        "print(onnx_config_for_seq_clf.outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8hPCv74Ylcje"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "onnx_inputs, onnx_outputs = export(\n",
        "        tokenizer,\n",
        "        model.to(device),\n",
        "        onnx_config_for_seq_clf,\n",
        "        output=Path(\"output/onnx_transforms/rubert-base-cased-sentiment.onnx\"),\n",
        "        opset=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если проверить скорость инференса и качество, получим то же самое, так что проверим только на gpu"
      ],
      "metadata": {
        "id": "WhrVYCm5w96r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IvtrvbmqxCT7"
      },
      "outputs": [],
      "source": [
        "providers = [\n",
        "    'CUDAExecutionProvider'\n",
        "]\n",
        "model_ONNX = nxrun.InferenceSession(\"output/onnx_transforms/rubert-base-cased-sentiment.onnx\", sess_options, providers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3621aff5-6f2b-4418-fc7f-aba4d7c208a6",
        "id": "REK315RPxCT8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 4.89 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "predict_onnx(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_onnx(t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7151d7-15e1-4688-b1f5-21df25bc0317",
        "id": "Y-7g0KH1xLXL"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.779944967747291, recall: 0.7718710037195792, f1score: 0.7701675746159881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TorchScript"
      ],
      "metadata": {
        "id": "6QZEJMl0exff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchScript — инструмент, который позволяет с помощью пары строк кода и нескольких щелчков мыши сделать из пайплайна на питоне отчуждаемое решение, которое можно встроить в систему на C++. А еще она будет на python работать быстрее из-за jit компиляции. В библиотеке transformers так же [есть почти из коробки](https://huggingface.co/docs/transformers/serialization#torchscript)"
      ],
      "metadata": {
        "id": "OOFNgikneVzn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "z_Gx7ViOnovm"
      },
      "outputs": [],
      "source": [
        "tokenizer_torchscript = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment', torchscript = True)\n",
        "model_torchscript = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment', return_dict=True, torchscript=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tSjgRCg-nbhs"
      },
      "outputs": [],
      "source": [
        "dummy_input0 = torch.randint(1, 224, (1,512))\n",
        "dummy_input1 = torch.randint(0, 1, (1,512 ))\n",
        "dummy_input2 =  torch.randint(0, 1, (1,512 ))\n",
        "traced_model = torch.jit.trace(model_torchscript, [x.clone().detach() for x in dummy_inputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "rN15KXu3s_ZX"
      },
      "outputs": [],
      "source": [
        "!mkdir -p output/torchscript"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.jit.save(traced_model, \"output/torchscript/rubert-base-cased-sentiment_traced.pt\")"
      ],
      "metadata": {
        "id": "fWRFFZt1gMPP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMMMa85wtFU5"
      },
      "source": [
        "Пробуем загрузить и предиктить на gpu и cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traced_model = torch.jit.load(\"output/torchscript/rubert-base-cased-sentiment_traced.pt\")"
      ],
      "metadata": {
        "id": "h7HUSg2wy2AE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "gkJz0sciq9K5"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_torchscript(text, device):\n",
        "    local_model = traced_model.to(device)\n",
        "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    outputs = local_model(**inputs)[0]\n",
        "    predicted = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    predicted = torch.argmax(predicted, dim=1).cpu().numpy()\n",
        "    return predicted[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "bXfsa0HOtY-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b27968-4871-4378-8ecf-728c6826c4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.40 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 101 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "predict_torchscript(text = text, device = torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "predict_torchscript(text = text, device = torch.device('cuda'))"
      ],
      "metadata": {
        "id": "yzRS_gFWC61j",
        "outputId": "67bce7a2-4d8d-470b-b406-2ebccf1ed155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 22.61 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 10 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем качество модели"
      ],
      "metadata": {
        "id": "-wlPWFDfghd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_torchscript(text=t, device = torch.device('cuda')) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "id": "FW3k65H6gj7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac70d25-1d66-4d74-c38c-69bb674d7cbf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.779944967747291, recall: 0.7718710037195792, f1score: 0.7701675746159881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Посмотрим на вес модели"
      ],
      "metadata": {
        "id": "xBMIZs9_i907"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "WRTog6pvuSL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d7d696-ecf1-4c88-c18a-c218aa405443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679M\toutput/torchscript/rubert-base-cased-sentiment_traced.pt\n",
            "679M\ttotal\n"
          ]
        }
      ],
      "source": [
        "!du -shc output/torchscript/*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:  по сравнению с оригинальной моделью скорость инференса модели стала немного выше, метрики качества не изменились. Лучше, чем ничего. А вообще надо а с++ запускать, чтобы увидеть адекватный результат"
      ],
      "metadata": {
        "id": "glxV-dR4jfW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Прунинг модели"
      ],
      "metadata": {
        "id": "7jV0b67TjnwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Pruning — обрезание избыточных частей сети для ускорения инференса без потери точности. Наглядно — откуда, сколько и как можно вырезать."
      ],
      "metadata": {
        "id": "KslsGyjYkFes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Есть очень много способов прунинга моделей, но здесь мы рассмотрим способ прунинга attention слоев"
      ],
      "metadata": {
        "id": "0PyJxe0RaJ4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Есть два варианта, как прунить модель.\n",
        "\n",
        "1 вариант - делать через torch.nn.utils.prune. В качестве примера есть данный [ноутбук](https://github.com/Huffon/nlp-various-tutorials/blob/master/pruning-bert.ipynb)\n",
        "\n",
        "2 вариант - библиотека [nn_pruning](https://github.com/huggingface/nn_pruning) от HuggingFace\n"
      ],
      "metadata": {
        "id": "QQecIsnQx431"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTwYCLrvtsi"
      },
      "source": [
        "[Ссылка](https://aclanthology.org/2020.repl4nlp-1.18.pdf) на почитать про прунинг модели BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMxrxaTf9yCy"
      },
      "source": [
        "Посмотрим на саму модель. Видим, что большую ее часть занимают attention слои. Давайте их и запруним и посмотрим, что получится"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "laqVNM52oYsS",
        "outputId": "f6a3f89a-b93e-461f-9c64-0a79b616ec81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "aunSquBo4Qr9"
      },
      "outputs": [],
      "source": [
        "pruned_model = model\n",
        "\n",
        "parameters_to_prune = ()\n",
        "for i in range(12):\n",
        "    parameters_to_prune += (\n",
        "        (pruned_model.bert.encoder.layer[i].attention.self.key, 'weight'),\n",
        "        (pruned_model.bert.encoder.layer[i].attention.self.query, 'weight'),\n",
        "        (pruned_model.bert.encoder.layer[i].attention.self.value, 'weight'),\n",
        "    )\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.8\n",
        ")\n",
        "for p in parameters_to_prune:\n",
        "  prune.remove(p[0], 'weight')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем, что получилось"
      ],
      "metadata": {
        "id": "AVBZ5em9y7y7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "brkL0Q0u72EK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfb2271-8a0d-4e63-ce0b-60e85393b5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in Layer 1-th key weight: 77.23%\n",
            "Sparsity in Layer 1-th query weightt: 77.17%\n",
            "Sparsity in Layer 1-th value weight: 91.60%\n",
            "\n",
            "Sparsity in Layer 2-th key weight: 77.46%\n",
            "Sparsity in Layer 2-th query weightt: 76.82%\n",
            "Sparsity in Layer 2-th value weight: 90.37%\n",
            "\n",
            "Sparsity in Layer 3-th key weight: 80.21%\n",
            "Sparsity in Layer 3-th query weightt: 79.54%\n",
            "Sparsity in Layer 3-th value weight: 87.01%\n",
            "\n",
            "Sparsity in Layer 4-th key weight: 77.59%\n",
            "Sparsity in Layer 4-th query weightt: 77.35%\n",
            "Sparsity in Layer 4-th value weight: 88.18%\n",
            "\n",
            "Sparsity in Layer 5-th key weight: 76.99%\n",
            "Sparsity in Layer 5-th query weightt: 76.74%\n",
            "Sparsity in Layer 5-th value weight: 85.86%\n",
            "\n",
            "Sparsity in Layer 6-th key weight: 76.24%\n",
            "Sparsity in Layer 6-th query weightt: 75.17%\n",
            "Sparsity in Layer 6-th value weight: 82.86%\n",
            "\n",
            "Sparsity in Layer 7-th key weight: 77.00%\n",
            "Sparsity in Layer 7-th query weightt: 75.85%\n",
            "Sparsity in Layer 7-th value weight: 83.59%\n",
            "\n",
            "Sparsity in Layer 8-th key weight: 77.44%\n",
            "Sparsity in Layer 8-th query weightt: 76.97%\n",
            "Sparsity in Layer 8-th value weight: 81.95%\n",
            "\n",
            "Sparsity in Layer 9-th key weight: 76.54%\n",
            "Sparsity in Layer 9-th query weightt: 76.23%\n",
            "Sparsity in Layer 9-th value weight: 84.85%\n",
            "\n",
            "Sparsity in Layer 10-th key weight: 77.35%\n",
            "Sparsity in Layer 10-th query weightt: 76.76%\n",
            "Sparsity in Layer 10-th value weight: 83.97%\n",
            "\n",
            "Sparsity in Layer 11-th key weight: 77.69%\n",
            "Sparsity in Layer 11-th query weightt: 77.06%\n",
            "Sparsity in Layer 11-th value weight: 85.45%\n",
            "\n",
            "Sparsity in Layer 12-th key weight: 77.73%\n",
            "Sparsity in Layer 12-th query weightt: 77.04%\n",
            "Sparsity in Layer 12-th value weight: 82.12%\n",
            "\n",
            "Global sparsity: 80.00%\n"
          ]
        }
      ],
      "source": [
        "for i in range(12):\n",
        "    print(\n",
        "        \"Sparsity in Layer {}-th key weight: {:.2f}%\".format(\n",
        "            i+1,\n",
        "            100. * float(torch.sum(pruned_model.bert.encoder.layer[i].attention.self.key.weight == 0))\n",
        "            / float(pruned_model.bert.encoder.layer[i].attention.self.key.weight.nelement())\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"Sparsity in Layer {}-th query weightt: {:.2f}%\".format(\n",
        "            i+1,\n",
        "            100. * float(torch.sum(pruned_model.bert.encoder.layer[i].attention.self.query.weight == 0))\n",
        "            / float(pruned_model.bert.encoder.layer[i].attention.self.query.weight.nelement())\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"Sparsity in Layer {}-th value weight: {:.2f}%\".format(\n",
        "            i+1,\n",
        "            100. * float(torch.sum(pruned_model.bert.encoder.layer[i].attention.self.value.weight == 0))\n",
        "            / float(pruned_model.bert.encoder.layer[i].attention.self.value.weight.nelement())\n",
        "        )\n",
        "    )\n",
        "    print()\n",
        "\n",
        "    \n",
        "numerator, denominator = 0, 0\n",
        "for i in range(12):\n",
        "    numerator += torch.sum(pruned_model.bert.encoder.layer[i].attention.self.key.weight == 0)\n",
        "    numerator += torch.sum(pruned_model.bert.encoder.layer[i].attention.self.query.weight == 0)\n",
        "    numerator += torch.sum(pruned_model.bert.encoder.layer[i].attention.self.value.weight == 0)\n",
        "\n",
        "    denominator += pruned_model.bert.encoder.layer[i].attention.self.key.weight.nelement()\n",
        "    denominator += pruned_model.bert.encoder.layer[i].attention.self.query.weight.nelement()\n",
        "    denominator += pruned_model.bert.encoder.layer[i].attention.self.value.weight.nelement()\n",
        "    \n",
        "print(\"Global sparsity: {:.2f}%\".format(100. * float(numerator) / float(denominator)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0XqK9i9Wqx"
      },
      "source": [
        "Предиктим на запруненной модели на gpu и cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5DX9s0-a9VW7"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_pruned(text, device):\n",
        "    local_model = pruned_model.to(device)\n",
        "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    outputs = local_model(**inputs)\n",
        "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    predicted = torch.argmax(predicted, dim=1).cpu().numpy()\n",
        "    return predicted[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "oKkXd1Yc9jtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bd83c1-4bd4-426f-e427-544cc557fdf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 150 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "predict_pruned(text, device = torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "predict_pruned(text, device = torch.device('cuda'))"
      ],
      "metadata": {
        "id": "keJst4-ADc6D",
        "outputId": "f3d9b6a5-228d-4740-e031-77f75d6534f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 16.14 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 21.6 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "tmheSjAr9rt2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p output/pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "S6-5H9Cq-k9m"
      },
      "outputs": [],
      "source": [
        "torch.save(pruned_model, 'output/pruning/rubert-base-cased-sentiment_pruned.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем качество модели"
      ],
      "metadata": {
        "id": "jk8IWvdqzaiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_pruned(text = t, device=torch.device('cuda')) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "id": "tiTTVRFBzqOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61cd2530-3d5f-48c6-f950-04f0f3f88666"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.805674989730417, recall: 0.7873728838562007, f1score: 0.7917501120030411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Посмотрим на вес модели"
      ],
      "metadata": {
        "id": "N1voUhql0oZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "DzLIjxUd0oZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0508dc88-0a95-4aba-a9a9-972dc368ddce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679M\toutput/pruning/rubert-base-cased-sentiment_pruned.pt\n",
            "679M\ttotal\n"
          ]
        }
      ],
      "source": [
        "!du -shc output/pruning/*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод - вес модели не поменялся, качество даже чуть-чуть улучшилось по сравнению с оригинальной моделью, но не критично"
      ],
      "metadata": {
        "id": "5r9o1Bhk04UB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Квантизация"
      ],
      "metadata": {
        "id": "ygi6u3xv3nK_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDQnTXSifdnm"
      },
      "source": [
        "Квантизация означает уменьшение численной точности весов модели. Один из популярных методов — k-means квантизация. Имея веса модели в матрице W с десятичными числами, веса кластеризуются с помощью k-means в N кластеров. Затем матрица W трансформируется в матрицу целых чисел от 1 до N, каждое из которых является указателем к центру кластера. Так можно сжать каждый элемент изначальной матрицы из 32-битного десятичного числа в log(N)-битные целые числа.\n",
        "\n",
        "Есть три вида квантизации - статическая, динамическая и Quantization-Aware-Training(QAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3MXWSQf1bF"
      },
      "source": [
        "Динамическая квантизация не требует ничего, поэтому она самая простая"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "SIteomPhgK3j"
      },
      "outputs": [],
      "source": [
        "!mkdir -p output/quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно квантизировать модель через библиотеку onnxruntime \n",
        "(!Внимание, операция очень тяжелая, вам может не хватить памяти в колабе, лучше запускать отдельно. Так же onnxruntime не работает с квантизированными моделями на gpu, потому запустится она только на cpu)"
      ],
      "metadata": {
        "id": "zP9ACDI5yVLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = 'output/onnx_transforms/rubert-base-cased-sentiment_torch.onnx'\n",
        "model_quant = 'output/quantization/rubert-base-cased-sentiment.quant.onnx'\n",
        "quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)"
      ],
      "metadata": {
        "id": "U3TcKmpmyaRv",
        "outputId": "a0a770e2-a795-4db1-8ba3-c38ceceeb827",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignore MatMul due to non constant B: /[MatMul_68]\n",
            "Ignore MatMul due to non constant B: /[MatMul_73]\n",
            "Ignore MatMul due to non constant B: /[MatMul_162]\n",
            "Ignore MatMul due to non constant B: /[MatMul_167]\n",
            "Ignore MatMul due to non constant B: /[MatMul_256]\n",
            "Ignore MatMul due to non constant B: /[MatMul_261]\n",
            "Ignore MatMul due to non constant B: /[MatMul_350]\n",
            "Ignore MatMul due to non constant B: /[MatMul_355]\n",
            "Ignore MatMul due to non constant B: /[MatMul_444]\n",
            "Ignore MatMul due to non constant B: /[MatMul_449]\n",
            "Ignore MatMul due to non constant B: /[MatMul_538]\n",
            "Ignore MatMul due to non constant B: /[MatMul_543]\n",
            "Ignore MatMul due to non constant B: /[MatMul_632]\n",
            "Ignore MatMul due to non constant B: /[MatMul_637]\n",
            "Ignore MatMul due to non constant B: /[MatMul_726]\n",
            "Ignore MatMul due to non constant B: /[MatMul_731]\n",
            "Ignore MatMul due to non constant B: /[MatMul_820]\n",
            "Ignore MatMul due to non constant B: /[MatMul_825]\n",
            "Ignore MatMul due to non constant B: /[MatMul_914]\n",
            "Ignore MatMul due to non constant B: /[MatMul_919]\n",
            "Ignore MatMul due to non constant B: /[MatMul_1008]\n",
            "Ignore MatMul due to non constant B: /[MatMul_1013]\n",
            "Ignore MatMul due to non constant B: /[MatMul_1102]\n",
            "Ignore MatMul due to non constant B: /[MatMul_1107]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--7npahjgBwa"
      },
      "source": [
        "Пробуем запустить динамечески квантизированную ONNX модель и посмотреть на время инференса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DcPhqkvdgPZv"
      },
      "outputs": [],
      "source": [
        "sess_options = nxrun.SessionOptions()\n",
        "providers = [\n",
        "    'CPUExecutionProvider'\n",
        "]\n",
        "\n",
        "model_ONNX = nxrun.InferenceSession(\"output/quantization/rubert-base-cased-sentiment.quant.onnx\", sess_options, providers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ab-IIy12gSnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a985b7-0b2f-4468-a1fb-4401cca65907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 37.9 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "predict_onnx(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем качество"
      ],
      "metadata": {
        "id": "EQ0Agafp43uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_onnx(t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "id": "xuQ36XHV43uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4400e397-9a50-402f-be38-f6f3e2069772"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7545168261175811, recall: 0.7505926047060564, f1score: 0.7370407334951347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно так же через библиотеку optimum от transformers"
      ],
      "metadata": {
        "id": "YLil08WNY9Z2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "D5h7RVrMfUnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c986d1-43d5-4fee-d2a1-aa6901122901"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('output/quantization/rubert-base-cased-sentiment_dyn_quantized.onnx')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# The type of quantization to apply\n",
        "qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n",
        "quantizer = ORTQuantizer.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\", feature=\"sequence-classification\")\n",
        "\n",
        "# Quantize the model!\n",
        "quantizer.export(\n",
        "    onnx_model_path=\"output/quantization/rubert-base-cased-sentiment.onnx\",\n",
        "    onnx_quantized_model_output_path=\"output/quantization/rubert-base-cased-sentiment_dyn_quantized.onnx\",\n",
        "    quantization_config=qconfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на вес модели"
      ],
      "metadata": {
        "id": "EhFEQrOw43uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!du -shc output/quantization/*"
      ],
      "metadata": {
        "id": "1BWVnSpI43uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807a60f2-d5f2-4e7e-fdaa-17cf479a4489"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "436M\toutput/quantization/rubert-base-cased-sentiment_dyn_quantized.onnx\n",
            "679M\toutput/quantization/rubert-base-cased-sentiment.onnx\n",
            "171M\toutput/quantization/rubert-base-cased-sentiment.quant.onnx\n",
            "1.3G\ttotal\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of ways of convert rubert_sentiment_classification.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/starminalush/mlops_report/blob/main/ways_of_convert_rubert_sentiment_classification.ipynb",
      "authorship_tag": "ABX9TyMhcuV3IpD4vMgLEuGQTKdB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}