{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starminalush/mlops_report/blob/main/ways_of_convert_rubert_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Введение"
      ],
      "metadata": {
        "id": "NCqsfpgGKl0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот ноутбук для бекендеров, которым дали модельку и сказали деплоить так, чтобы она быстро работала. И больше ничего не дали, кроме модельки\n",
        "\t\n",
        "  (・_・ヾ"
      ],
      "metadata": {
        "id": "swO2xuTUxKuj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIhzpUzi6O93"
      },
      "source": [
        "Устанавливаем нужные зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98Fnu0cUxT-B",
        "outputId": "0161ff10-8d18-4fb2-a88b-f9e67ae9c80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 32.0 MB/s \n",
            "\u001b[?25hCollecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.11.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (108.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 108.9 MB 46 kB/s \n",
            "\u001b[?25hCollecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting optimum[onnxruntime]\n",
            "  Downloading optimum-1.1.1.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 2.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 31.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime-gpu) (2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.7/dist-packages (from optimum[onnxruntime]) (1.11.0+cu113)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from optimum[onnxruntime]) (1.7.1)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.11.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 38.0 MB/s \n",
            "\u001b[?25hCollecting datasets>=1.2.1\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.3.4)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 46.4 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (6.0.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 44.6 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2022.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->optimum[onnxruntime]) (1.2.1)\n",
            "Building wheels for collected packages: folium, optimum, sacremoses\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=7886c84a261baccb1b361202ef0ca0b1a73a926ff4a5a5ed90dc8f8246b24704\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "  Building wheel for optimum (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum: filename=optimum-1.1.1-py3-none-any.whl size=83821 sha256=dae1771477dd513733fd535719d0a90b67f49b9aa975bfbff26ac6a34f33413c\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/fd/0e/f2d8b5936fd9a3b231b0146ad0233d0572879e5a22378c9a16\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=6ff3b34f0c5a34e6214581358f3822e90de4fdf175821db4208cd0070becfd59\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built folium optimum sacremoses\n",
            "Installing collected packages: urllib3, multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, sacremoses, humanfriendly, huggingface-hub, fsspec, aiohttp, xxhash, transformers, responses, coloredlogs, optimum, onnxruntime, onnx, datasets, onnxruntime-gpu, folium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 coloredlogs-15.0.1 datasets-2.1.0 folium-0.2.1 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 humanfriendly-10.0 multidict-6.0.2 onnx-1.11.0 onnxruntime-1.11.1 onnxruntime-gpu-1.11.1 optimum-1.1.1 pyyaml-6.0 responses-0.18.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx transformers onnxruntime-gpu folium==0.2.1 optimum[onnxruntime]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2LrH-YLj6jK"
      },
      "source": [
        "Фиксируем версии библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RJtoXVKbj43r"
      },
      "outputs": [],
      "source": [
        "!pip freeze > req.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX3P6Agw6jsm"
      },
      "source": [
        "Импорты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UPvsKtuB6iyY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers.onnx import export\n",
        "from pathlib import Path\n",
        "from typing import Mapping, OrderedDict\n",
        "from transformers.onnx import OnnxConfig\n",
        "from transformers import AutoConfig\n",
        "import onnxruntime as nxrun\n",
        "import onnx\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "from torch.nn.utils import prune\n",
        "from optimum.onnxruntime import ORTQuantizer\n",
        "from torch.onnx import TrainingMode\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Качаем датасет, на котором будем проверять качество модели"
      ],
      "metadata": {
        "id": "PeVa2PBwZ-63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/sismetanin/rureviews/raw/master/women-clothing-accessories.3-class.balanced.csv"
      ],
      "metadata": {
        "id": "bswJJzJwZ-ZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6adcca0-9b1d-4ae4-e0f8-a8de5f53ee20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-03 14:00:11--  https://github.com/sismetanin/rureviews/raw/master/women-clothing-accessories.3-class.balanced.csv\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sismetanin/rureviews/master/women-clothing-accessories.3-class.balanced.csv [following]\n",
            "--2022-05-03 14:00:12--  https://raw.githubusercontent.com/sismetanin/rureviews/master/women-clothing-accessories.3-class.balanced.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21781685 (21M) [text/plain]\n",
            "Saving to: ‘women-clothing-accessories.3-class.balanced.csv’\n",
            "\n",
            "women-clothing-acce 100%[===================>]  20.77M  88.8MB/s    in 0.2s    \n",
            "\n",
            "2022-05-03 14:00:13 (88.8 MB/s) - ‘women-clothing-accessories.3-class.balanced.csv’ saved [21781685/21781685]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Об нейросети"
      ],
      "metadata": {
        "id": "bN3PMwCwKu8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве подопытного будем использовать [rubert-base-cased-sentiment](https://huggingface.co/blanchefort/rubert-base-cased-sentiment) для классификации русских предложений. Данная нейросеть предсказывает 3 метки класса, в зависимости от тона предложения - позитивное, негативное или нейтральное"
      ],
      "metadata": {
        "id": "OnWsmqJ8JEID"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCV1_Y3hjr4C"
      },
      "source": [
        "Запускаем нейросеть как есть"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device  = torch.device('cuda') #будем все запускать на gpu"
      ],
      "metadata": {
        "id": "WAlMkNC-MUBc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y68sxMotjS-K"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment', return_dict=True).to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    outputs = model(**inputs)\n",
        "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    predicted = torch.argmax(predicted, dim=1).cpu().numpy()\n",
        "    return predicted[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Как задолбали эти тупые правила: не есть кота, не бить посуду, не есть кота'"
      ],
      "metadata": {
        "id": "j3WKSWz8bsoJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим время инференса модели"
      ],
      "metadata": {
        "id": "tj7kkrS1R2KL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "halOX7lokqyT",
        "outputId": "3a813f13-5a93-43bc-e1c3-d24d898c3cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25.4 ms, sys: 775 µs, total: 26.2 ms\n",
            "Wall time: 25.9 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "%%time\n",
        "predict(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим качество модели. Для проверки качества будем использовать один из датасетов, на котором обучалась модель, а именно [этот](https://github.com/sismetanin/rureviews)"
      ],
      "metadata": {
        "id": "jze-C8D-R6jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/women-clothing-accessories.3-class.balanced.csv', delimiter='\\t')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Blvw3zQqVSZS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0cf2ae93-23e6-425a-d5b0-f10c55bdc355"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  качество плохое пошив ужасный (горловина напер...  negative\n",
              "1  Товар отдали другому человеку, я не получила п...  negative\n",
              "2  Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
              "3  товар не пришел, продавец продлил защиту без м...  negative\n",
              "4      Кофточка голая синтетика, носить не возможно.  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dae3c0c6-6257-423b-a35e-5914f907cb09\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>качество плохое пошив ужасный (горловина напер...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Товар отдали другому человеку, я не получила п...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ужасная синтетика! Тонкая, ничего общего с пре...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>товар не пришел, продавец продлил защиту без м...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Кофточка голая синтетика, носить не возможно.</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dae3c0c6-6257-423b-a35e-5914f907cb09')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dae3c0c6-6257-423b-a35e-5914f907cb09 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dae3c0c6-6257-423b-a35e-5914f907cb09');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для удобства немного изменим датасет - заменим метки класса на цифровые значения и выберем 1000 рандомных строк"
      ],
      "metadata": {
        "id": "ul8Y52nOVx_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df = df[:1000]\n",
        "mapping = {'negative': 2, 'positive': 1, 'neautral':0}\n",
        "df = df.replace({'sentiment': mapping})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3csvwJ45V6SO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9274a0c6-3094-47e7-9d66-1f9a448bceef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  По составу ожидала лучшего, а так на 4. В Каза...          1\n",
              "1  Заказывала тёмно синею юбку , но мне пришла не...          0\n",
              "2  Очень долго шел, с задержкой на неделю, заказ ...          0\n",
              "3  блузка не очень, ткань плотная напоминает ткан...          0\n",
              "4  Товар пришёл в течении месяца. Всё по размеру,...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef4c654e-de78-47c8-9b50-d5835600ce92\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>По составу ожидала лучшего, а так на 4. В Каза...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Заказывала тёмно синею юбку , но мне пришла не...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Очень долго шел, с задержкой на неделю, заказ ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>блузка не очень, ткань плотная напоминает ткан...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Товар пришёл в течении месяца. Всё по размеру,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef4c654e-de78-47c8-9b50-d5835600ce92')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ef4c654e-de78-47c8-9b50-d5835600ce92 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ef4c654e-de78-47c8-9b50-d5835600ce92');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем качество"
      ],
      "metadata": {
        "id": "QwNFigwRX451"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(df['review'])\n",
        "labels = list(df['sentiment'])"
      ],
      "metadata": {
        "id": "sziEW5X3X8qD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict(t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZg4zSOAYSAk",
        "outputId": "0e352015-ccba-41c6-b307-06c37ec36b2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7610035916343683, recall: 0.7507866077468736, f1score: 0.7469645633008467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраним оригинальную модель и посмотрим на ее вес"
      ],
      "metadata": {
        "id": "YCTWXOk829Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output"
      ],
      "metadata": {
        "id": "uNKv-i2QcfGP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'output/original.pt')"
      ],
      "metadata": {
        "id": "mtpd-cj53F_V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -shc output/original.pt"
      ],
      "metadata": {
        "id": "Jy4Ok3jw3OUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d1c6ca-6281-4bbd-d097-3bca1b74c7c3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679M\toutput/original.pt\n",
            "679M\ttotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX"
      ],
      "metadata": {
        "id": "ITuXOm0CZ0Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формат Open Neural Network Exchange (ONNX) обеспечит общий способ представления данных, используемых в нейронных сетях. Большинство платформ имеют сегодня собственный специфический формат моделей, которые способны работать с моделями других платформ только при использовании специальных инструментов преобразования форматов.\n",
        "\n",
        "ONNX позволит осуществлять свободный обмен информацией, которой обладают модели, без процедуры преобразования. Модель, обученную на одной платформе, можно будет использовать и на другой платформе. Также можно будет модель, обученную на одном фреймворке, перенести на другой фреймворк.\n",
        "\n",
        "Перевести модель в ONNX можно несколькими способами:"
      ],
      "metadata": {
        "id": "p0aL7IAQaHje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Есть способ конвертации модели через torch.onnx"
      ],
      "metadata": {
        "id": "goPzxMVnadSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "S6uO6quNkUSn"
      },
      "outputs": [],
      "source": [
        "!mkdir -p output/onnx_transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#делаем dummy input\n",
        "dummy_input0 = torch.randint(1, 224, (1,512)).to(device)\n",
        "dummy_input1 = torch.randint(0, 1, (1,512 )).to(device)\n",
        "dummy_input2 =  torch.randint(0, 1, (1,512 )).to(device)\n",
        "dummy_inputs = (dummy_input0,dummy_input1,dummy_input2)\n",
        "with torch.no_grad():\n",
        "  symbolic_names = {0:'batch_size', 1: 'max_seq_len'} \n",
        "  torch.onnx.export(model,               # модель, которую будем экспортировать\n",
        "                    dummy_inputs,                         # input модели\n",
        "                    \"output/onnx_transforms/rubert-base-cased-sentiment_torch.onnx\",   # путь сохранения модель\n",
        "                    export_params=True,        \n",
        "                    opset_version=11,          # версия ONNX, в который будем экспортировать модель\n",
        "                    do_constant_folding=True,\n",
        "                    input_names = [\"input_ids\",\"attention_mask\",\"token_type_ids\"],\n",
        "                    output_names = ['output'],\n",
        "                    dynamic_axes={'input_ids': symbolic_names,        #если у нас динамический размер input\n",
        "                                  'attention_mask' : symbolic_names,\n",
        "                                  'token_type_ids' : symbolic_names},\n",
        "                    training=TrainingMode.EVAL\n",
        "                    )"
      ],
      "metadata": {
        "id": "AsPlz8DGauZV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMKViWTow1s_"
      },
      "source": [
        "Пробуем запустить в ONNX и посмотреть время инференса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CeVlwXC8xbpj"
      },
      "outputs": [],
      "source": [
        "sess_options = nxrun.SessionOptions()\n",
        "providers = [\n",
        "    'CUDAExecutionProvider'\n",
        "]\n",
        "\n",
        "model_ONNX = nxrun.InferenceSession(\"output/onnx_transforms/rubert-base-cased-sentiment_torch.onnx\", sess_options, providers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iAu2HFt6x9WM"
      },
      "outputs": [],
      "source": [
        "def predict_onnx(text):\n",
        "  inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='np')\n",
        "  outputs  = model_ONNX.run(None, dict(inputs))[0][0]\n",
        "  result = np.where(outputs == np.amax(outputs))[0][0]\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "d76ecT6N0a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742a5228-1764-49c4-e335-3e2f3062202f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14.7 ms, sys: 0 ns, total: 14.7 ms\n",
            "Wall time: 17.7 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "%%time\n",
        "predict_onnx(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем качество"
      ],
      "metadata": {
        "id": "rdcziw6HeR6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_onnx(t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "id": "_t96PNPob8I0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ca90a2-0adf-4c5f-e01e-20b48c84ee27"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7610035916343683, recall: 0.7507866077468736, f1score: 0.7469645633008467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на вес модели"
      ],
      "metadata": {
        "id": "UrZPxeh_jCk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!du -shc output/onnx_transforms/*"
      ],
      "metadata": {
        "id": "BXjx9u88jFDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7be877-fd4e-48e2-b81e-085b9e90b663"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679M\toutput/onnx_transforms/rubert-base-cased-sentiment_torch.onnx\n",
            "679M\ttotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:  по сравнению с оригинальной моделью скорость инференса модели стала на порядок выше, метрики качества не изменились"
      ],
      "metadata": {
        "id": "ncawUB5neW9b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhA84H-tlJCl"
      },
      "source": [
        "2. Есть библиотека transforms для трансформеров, [где все почти из коробки](https://huggingface.co/docs/transformers/serialization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dXO_0DXu3e3s"
      },
      "outputs": [],
      "source": [
        "class DistilBertOnnxConfig(OnnxConfig):\n",
        "    @property\n",
        "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
        "        return OrderedDict(\n",
        "            [\n",
        "                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n",
        "                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n",
        "                (\"token_type_ids\", {0: \"batch\", 1: \"sequence\"}),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kFThnLqO3O6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df874444-e8ae-4956-824d-b6d0d7be8447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('logits', {0: 'batch'})])\n"
          ]
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\")\n",
        "onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task=\"sequence-classification\")\n",
        "print(onnx_config_for_seq_clf.outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8hPCv74Ylcje"
      },
      "outputs": [],
      "source": [
        "onnx_inputs, onnx_outputs = export(\n",
        "        tokenizer,\n",
        "        model,\n",
        "        onnx_config_for_seq_clf,\n",
        "        output=Path(\"output/onnx_transforms/rubert-base-cased-sentiment.onnx\"),\n",
        "        opset=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если проверить скорость инференса и качество, получим то же самое"
      ],
      "metadata": {
        "id": "WhrVYCm5w96r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IvtrvbmqxCT7"
      },
      "outputs": [],
      "source": [
        "model_ONNX = nxrun.InferenceSession(\"output/onnx_transforms/rubert-base-cased-sentiment.onnx\", sess_options, providers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecc3f8d-698d-4280-d182-7f1224f4824d",
        "id": "REK315RPxCT8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.3 ms, sys: 1.77 ms, total: 17.1 ms\n",
            "Wall time: 17.9 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "%%time\n",
        "predict_onnx(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_onnx(t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca9834f-b541-4f94-d581-334061976819",
        "id": "Y-7g0KH1xLXL"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7610035916343683, recall: 0.7507866077468736, f1score: 0.7469645633008467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TorchScript"
      ],
      "metadata": {
        "id": "6QZEJMl0exff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchScript — инструмент, который позволяет с помощью пары строк кода и нескольких щелчков мыши сделать из пайплайна на питоне отчуждаемое решение, которое можно встроить в систему на C++. А еще она будет на python работать быстрее из-за jit компиляции. В библиотеке transformers так же [есть почти из коробки](https://huggingface.co/docs/transformers/serialization#torchscript)"
      ],
      "metadata": {
        "id": "OOFNgikneVzn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "z_Gx7ViOnovm"
      },
      "outputs": [],
      "source": [
        "tokenizer_torchscript = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment', torchscript = True)\n",
        "model_torchscript = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment', return_dict=True, torchscript=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tSjgRCg-nbhs"
      },
      "outputs": [],
      "source": [
        "dummy_input0 = torch.randint(1, 224, (1,512)).to(device)\n",
        "dummy_input1 = torch.randint(0, 1, (1,512 )).to(device)\n",
        "dummy_input2 =  torch.randint(0, 1, (1,512 )).to(device)\n",
        "traced_model = torch.jit.trace(model_torchscript, [x.clone().detach() for x in dummy_inputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "rN15KXu3s_ZX"
      },
      "outputs": [],
      "source": [
        "!mkdir -p output/torchscript"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.jit.save(traced_model, \"output/torchscript/rubert-base-cased-sentiment_traced.pt\")"
      ],
      "metadata": {
        "id": "fWRFFZt1gMPP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMMMa85wtFU5"
      },
      "source": [
        "Пробуем загрузить и предиктить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "gkJz0sciq9K5"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_torchscript(text):\n",
        "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    outputs = traced_model(**inputs)[0]\n",
        "    predicted = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    predicted = torch.argmax(predicted, dim=1).cpu().numpy()\n",
        "    return predicted[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "bXfsa0HOtY-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7ae6b3-4581-48dc-f9e6-e997cb619b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21.8 ms, sys: 1.82 ms, total: 23.7 ms\n",
            "Wall time: 21.2 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "%%time\n",
        "predict_torchscript(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем качество модели"
      ],
      "metadata": {
        "id": "-wlPWFDfghd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_torchscript(t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "id": "FW3k65H6gj7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79596d7-c6e2-485f-c17d-c183d19fd563"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7610035916343683, recall: 0.7507866077468736, f1score: 0.7469645633008467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Посмотрим на вес модели"
      ],
      "metadata": {
        "id": "xBMIZs9_i907"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "WRTog6pvuSL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2150f2-c19f-4645-a207-f1c74ce13063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679M\toutput/torchscript/rubert-base-cased-sentiment_traced.pt\n",
            "679M\ttotal\n"
          ]
        }
      ],
      "source": [
        "!du -shc output/torchscript/*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:  по сравнению с оригинальной моделью скорость инференса модели стала немного выше, метрики качества не изменились. Лучше, чем ничего. А вообще надо а с++ запускать, чтобы увидеть адекватный результат"
      ],
      "metadata": {
        "id": "glxV-dR4jfW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Прунинг модели"
      ],
      "metadata": {
        "id": "7jV0b67TjnwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Pruning — обрезание избыточных частей сети для ускорения инференса без потери точности. Наглядно — откуда, сколько и как можно вырезать."
      ],
      "metadata": {
        "id": "KslsGyjYkFes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Есть очень много способов прунинга моделей, но здесь мы рассмотрим способ прунинга attention слоев"
      ],
      "metadata": {
        "id": "0PyJxe0RaJ4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Есть два варианта, как прунить модель.\n",
        "\n",
        "1 вариант - делать через torch.nn.utils.prune. В качестве примера есть данный [ноутбук](https://github.com/Huffon/nlp-various-tutorials/blob/master/pruning-bert.ipynb)\n",
        "\n",
        "2 вариант - библиотека [nn_pruning](https://github.com/huggingface/nn_pruning) от HuggingFace\n"
      ],
      "metadata": {
        "id": "QQecIsnQx431"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTwYCLrvtsi"
      },
      "source": [
        "[Ссылка](https://aclanthology.org/2020.repl4nlp-1.18.pdf) на почитать про прунинг модели BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMxrxaTf9yCy"
      },
      "source": [
        "Запруним encoder слои"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aunSquBo4Qr9"
      },
      "outputs": [],
      "source": [
        "pruned_model = model\n",
        "\n",
        "parameters_to_prune = ()\n",
        "for i in range(12):\n",
        "    parameters_to_prune += (\n",
        "        (pruned_model.bert.encoder.layer[i].attention.self.key, 'weight'),\n",
        "        (pruned_model.bert.encoder.layer[i].attention.self.query, 'weight'),\n",
        "        (pruned_model.bert.encoder.layer[i].attention.self.value, 'weight'),\n",
        "    )\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.4,\n",
        ")\n",
        "for p in parameters_to_prune:\n",
        "  prune.remove(p[0], 'weight')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем, что получилось"
      ],
      "metadata": {
        "id": "AVBZ5em9y7y7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brkL0Q0u72EK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ed872e-72c7-404b-c416-f92299e2185b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in Layer 1-th key weight: 89.01%\n",
            "Sparsity in Layer 1-th query weightt: 88.97%\n",
            "Sparsity in Layer 1-th value weight: 92.64%\n",
            "\n",
            "Sparsity in Layer 2-th key weight: 89.07%\n",
            "Sparsity in Layer 2-th query weightt: 88.86%\n",
            "Sparsity in Layer 2-th value weight: 92.28%\n",
            "\n",
            "Sparsity in Layer 3-th key weight: 89.71%\n",
            "Sparsity in Layer 3-th query weightt: 89.45%\n",
            "Sparsity in Layer 3-th value weight: 91.25%\n",
            "\n",
            "Sparsity in Layer 4-th key weight: 89.04%\n",
            "Sparsity in Layer 4-th query weightt: 88.94%\n",
            "Sparsity in Layer 4-th value weight: 91.66%\n",
            "\n",
            "Sparsity in Layer 5-th key weight: 88.94%\n",
            "Sparsity in Layer 5-th query weightt: 88.91%\n",
            "Sparsity in Layer 5-th value weight: 90.97%\n",
            "\n",
            "Sparsity in Layer 6-th key weight: 88.83%\n",
            "Sparsity in Layer 6-th query weightt: 88.68%\n",
            "Sparsity in Layer 6-th value weight: 90.33%\n",
            "\n",
            "Sparsity in Layer 7-th key weight: 88.96%\n",
            "Sparsity in Layer 7-th query weightt: 88.68%\n",
            "Sparsity in Layer 7-th value weight: 90.50%\n",
            "\n",
            "Sparsity in Layer 8-th key weight: 89.05%\n",
            "Sparsity in Layer 8-th query weightt: 88.96%\n",
            "Sparsity in Layer 8-th value weight: 89.98%\n",
            "\n",
            "Sparsity in Layer 9-th key weight: 88.79%\n",
            "Sparsity in Layer 9-th query weightt: 88.80%\n",
            "Sparsity in Layer 9-th value weight: 90.65%\n",
            "\n",
            "Sparsity in Layer 10-th key weight: 88.99%\n",
            "Sparsity in Layer 10-th query weightt: 88.89%\n",
            "Sparsity in Layer 10-th value weight: 90.38%\n",
            "\n",
            "Sparsity in Layer 11-th key weight: 89.00%\n",
            "Sparsity in Layer 11-th query weightt: 88.90%\n",
            "Sparsity in Layer 11-th value weight: 90.82%\n",
            "\n",
            "Sparsity in Layer 12-th key weight: 89.08%\n",
            "Sparsity in Layer 12-th query weightt: 88.78%\n",
            "Sparsity in Layer 12-th value weight: 89.99%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(12):\n",
        "    print(\n",
        "        \"Sparsity in Layer {}-th key weight: {:.2f}%\".format(\n",
        "            i+1,\n",
        "            100. * float(torch.sum(pruned_model.bert.encoder.layer[i].attention.self.key.weight == 0))\n",
        "            / float(pruned_model.bert.encoder.layer[i].attention.self.key.weight.nelement())\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"Sparsity in Layer {}-th query weightt: {:.2f}%\".format(\n",
        "            i+1,\n",
        "            100. * float(torch.sum(pruned_model.bert.encoder.layer[i].attention.self.query.weight == 0))\n",
        "            / float(pruned_model.bert.encoder.layer[i].attention.self.query.weight.nelement())\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"Sparsity in Layer {}-th value weight: {:.2f}%\".format(\n",
        "            i+1,\n",
        "            100. * float(torch.sum(pruned_model.bert.encoder.layer[i].attention.self.value.weight == 0))\n",
        "            / float(pruned_model.bert.encoder.layer[i].attention.self.value.weight.nelement())\n",
        "        )\n",
        "    )\n",
        "    print()\n",
        "\n",
        "    \n",
        "numerator, denominator = 0, 0\n",
        "for i in range(12):\n",
        "    numerator += torch.sum(pruned_model.bert.encoder.layer[i].attention.self.key.weight == 0)\n",
        "    numerator += torch.sum(pruned_model.bert.encoder.layer[i].attention.self.query.weight == 0)\n",
        "    numerator += torch.sum(pruned_model.bert.encoder.layer[i].attention.self.value.weight == 0)\n",
        "\n",
        "    denominator += pruned_model.bert.encoder.layer[i].attention.self.key.weight.nelement()\n",
        "    denominator += pruned_model.bert.encoder.layer[i].attention.self.query.weight.nelement()\n",
        "    denominator += pruned_model.bert.encoder.layer[i].attention.self.value.weight.nelement()\n",
        "    \n",
        "print(\"Global sparsity: {:.2f}%\".format(100. * float(numerator) / float(denominator)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0XqK9i9Wqx"
      },
      "source": [
        "Предиктим на запруненной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DX9s0-a9VW7"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_pruned(text):\n",
        "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "    outputs = pruned_model(**inputs)\n",
        "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    predicted = torch.argmax(predicted, dim=1).numpy()\n",
        "    return predicted[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKkXd1Yc9jtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6e444d-9ae9-4608-a0c8-bc32d4f91743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 113 ms, sys: 3.98 ms, total: 117 ms\n",
            "Wall time: 121 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "%%time\n",
        "predict_pruned(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmheSjAr9rt2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p output/pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6-5H9Cq-k9m"
      },
      "outputs": [],
      "source": [
        "torch.save(pruned_model, 'output/pruning/rubert-base-cased-sentiment_pruned.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем качество модели"
      ],
      "metadata": {
        "id": "jk8IWvdqzaiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_pruned(t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "id": "tiTTVRFBzqOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef217d49-1665-4b3a-b0e6-d09aefa43ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7578464700766858, recall: 0.35564648060211507, f1score: 0.2180023041110175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Посмотрим на вес модели"
      ],
      "metadata": {
        "id": "N1voUhql0oZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzLIjxUd0oZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac87199-a464-4de2-87a2-6383b80638a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679M\toutput/pruning/rubert-base-cased-sentiment_pruned.pt\n",
            "679M\ttotal\n"
          ]
        }
      ],
      "source": [
        "!du -shc output/pruning/*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод - вес модели не поменялся, качество упало чуть-чуть по сравнению с оригинальной моделью, но не критично. Модель стала работать быстрее, но ценой небольшой потери качества"
      ],
      "metadata": {
        "id": "5r9o1Bhk04UB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Квантизация"
      ],
      "metadata": {
        "id": "ygi6u3xv3nK_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDQnTXSifdnm"
      },
      "source": [
        "Квантизация означает уменьшение численной точности весов модели. Один из популярных методов — k-means квантизация. Имея веса модели в матрице W с десятичными числами, веса кластеризуются с помощью k-means в N кластеров. Затем матрица W трансформируется в матрицу целых чисел от 1 до N, каждое из которых является указателем к центру кластера. Так можно сжать каждый элемент изначальной матрицы из 32-битного десятичного числа в log(N)-битные целые числа.\n",
        "\n",
        "Есть три вида квантизации - статическая, динамическая и Quantization-Aware-Training(QAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3MXWSQf1bF"
      },
      "source": [
        "Динамическая квантизация не требует ничего, поэтому она самая простая"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIteomPhgK3j"
      },
      "outputs": [],
      "source": [
        "!mkdir -p output/quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно квантизировать модель через библиотеку onnxruntime "
      ],
      "metadata": {
        "id": "zP9ACDI5yVLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = 'output/onnx_transforms/rubert-base-cased-sentiment_torch.onnx'\n",
        "model_quant = 'output/quantization/rubert-base-cased-sentiment.quant.onnx'\n",
        "quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QInt8)"
      ],
      "metadata": {
        "id": "U3TcKmpmyaRv",
        "outputId": "94e8865a-0312-405b-d99d-2ebbfa6c55b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignore MatMul due to non constant B: /[MatMul_68]\n",
            "Ignore MatMul due to non constant B: /[MatMul_73]\n",
            "Ignore MatMul due to non constant B: /[MatMul_162]\n",
            "Ignore MatMul due to non constant B: /[MatMul_167]\n",
            "Ignore MatMul due to non constant B: /[MatMul_256]\n",
            "Ignore MatMul due to non constant B: /[MatMul_261]\n",
            "Ignore MatMul due to non constant B: /[MatMul_350]\n",
            "Ignore MatMul due to non constant B: /[MatMul_355]\n",
            "Ignore MatMul due to non constant B: /[MatMul_444]\n",
            "Ignore MatMul due to non constant B: /[MatMul_449]\n",
            "Ignore MatMul due to non constant B: /[MatMul_538]\n",
            "Ignore MatMul due to non constant B: /[MatMul_543]\n",
            "Ignore MatMul due to non constant B: /[MatMul_632]\n",
            "Ignore MatMul due to non constant B: /[MatMul_637]\n",
            "Ignore MatMul due to non constant B: /[MatMul_726]\n",
            "Ignore MatMul due to non constant B: /[MatMul_731]\n",
            "Ignore MatMul due to non constant B: /[MatMul_820]\n",
            "Ignore MatMul due to non constant B: /[MatMul_825]\n",
            "Ignore MatMul due to non constant B: /[MatMul_914]\n",
            "Ignore MatMul due to non constant B: /[MatMul_919]\n",
            "Ignore MatMul due to non constant B: /[MatMul_1008]\n",
            "Ignore MatMul due to non constant B: /[MatMul_1013]\n",
            "Ignore MatMul due to non constant B: /[MatMul_1102]\n",
            "Ignore MatMul due to non constant B: /[MatMul_1107]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--7npahjgBwa"
      },
      "source": [
        "Пробуем запустить динамечески квантизированную ONNX модель и посмотреть на время инференса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcPhqkvdgPZv"
      },
      "outputs": [],
      "source": [
        "sess_options = nxrun.SessionOptions()\n",
        "providers = [\n",
        "    'CPUExecutionProvider'\n",
        "]\n",
        "\n",
        "model_ONNX = nxrun.InferenceSession(\"output/quantization/rubert-base-cased-sentiment.quant.onnx\", sess_options, providers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab-IIy12gSnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80deaf91-39ec-481b-9f70-22a993e24021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 54 ms, sys: 0 ns, total: 54 ms\n",
            "Wall time: 54.4 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "%%time\n",
        "predict_onnx(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем качество"
      ],
      "metadata": {
        "id": "EQ0Agafp43uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [predict_onnx(t) for t in texts]\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "\n",
        "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "id": "xuQ36XHV43uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55dbe1b2-d66a-40ba-b8a0-b0a8f0ec7be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7575515032901955, recall: 0.7453613600241816, f1score: 0.7398248958145821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно так же через библиотеку optimum от transformers"
      ],
      "metadata": {
        "id": "YLil08WNY9Z2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5h7RVrMfUnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c737e3-2a6c-4d70-da82-72649955a6aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('output/quantization/rubert-base-cased-sentiment_dyn_quantized.onnx')"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "# The type of quantization to apply\n",
        "qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n",
        "quantizer = ORTQuantizer.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\", feature=\"sequence-classification\")\n",
        "\n",
        "# Quantize the model!\n",
        "quantizer.export(\n",
        "    onnx_model_path=\"output/quantization/rubert-base-cased-sentiment.onnx\",\n",
        "    onnx_quantized_model_output_path=\"output/quantization/rubert-base-cased-sentiment_dyn_quantized.onnx\",\n",
        "    quantization_config=qconfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на вес модели"
      ],
      "metadata": {
        "id": "EhFEQrOw43uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!du -shc output/quantization/*"
      ],
      "metadata": {
        "id": "1BWVnSpI43uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f50971-7e7e-4acc-bbe5-35c265d00e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "436M\toutput/quantization/rubert-base-cased-sentiment_dyn_quantized.onnx\n",
            "679M\toutput/quantization/rubert-base-cased-sentiment.onnx\n",
            "171M\toutput/quantization/rubert-base-cased-sentiment.quant.onnx\n",
            "1.3G\ttotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:  по сравнению с моделью в формате ONNX скорость инференса модели стала  выше, метрики качества немного просели.Вес модели не уменьшился"
      ],
      "metadata": {
        "id": "FJxqM9yu43uC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of ways of convert rubert_sentiment_classification.ipynb",
      "provenance": [],
      "mount_file_id": "1ce5RBtreotQMzJBUJz2mm9dv4BHlk1s0",
      "authorship_tag": "ABX9TyMPuf9evUk1f90Xdu+dveEz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}